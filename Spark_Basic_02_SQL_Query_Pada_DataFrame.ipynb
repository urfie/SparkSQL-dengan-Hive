{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SQL adalah salah satu bahasa populer untuk pemrosesan dan analisis data. Spark mendukung SQL untuk memproses DataFrame. Dalam latihan ini kita akan menggunakan spark SQL tanpa database.\n"
      ],
      "metadata": {
        "id": "vJbZRSkkTnAw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqGzlLOuBpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be56f10a-ddd2-499e-8d15-5e912806cf22"
      },
      "source": [
        "#%pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=4e1a7b1aaa7b7c592e098290795a772bedec046ea1b8f6834711e70f62098956\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJDNbI5gti9B"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inisialisasi spark session untuk berinteraksi dengan Spark cluster."
      ],
      "metadata": {
        "id": "HgEv2iNhTitC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnxed8rpt5Xs"
      },
      "source": [
        "spark = SparkSession.builder.appName('DataFrame Basics').getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset"
      ],
      "metadata": {
        "id": "iHGCn-voUTzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/application_record_header.csv.gz"
      ],
      "metadata": {
        "id": "PQTqnwezHtSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c67051a-f74f-404d-b84a-f0311366edb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-07 08:17:31--  https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/application_record_header.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/application_record_header.csv.gz [following]\n",
            "--2023-10-07 08:17:31--  https://raw.githubusercontent.com/urfie/SparkSQL-dengan-Hive/main/datasets/application_record_header.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3175443 (3.0M) [application/octet-stream]\n",
            "Saving to: ‘application_record_header.csv.gz’\n",
            "\n",
            "application_record_ 100%[===================>]   3.03M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-10-07 08:17:31 (34.2 MB/s) - ‘application_record_header.csv.gz’ saved [3175443/3175443]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load ke dataframe"
      ],
      "metadata": {
        "id": "-vEl7qY6UV4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"application_record_header.csv.gz\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "ArREYTQWHy3J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelum menggunakan SQL, kita perlu membuat temporary table dari dataframe yang akan kita olah.\n",
        "\n",
        "Gunakan fungsi `createOrReplaceTempView(nama_tabel)` pada dataframe tersebut."
      ],
      "metadata": {
        "id": "lvgyM2iKUYS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"app_record\")"
      ],
      "metadata": {
        "id": "29qI_YUJnF_N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya kita bisa menggunakan nama tabel yang sudah kita definisikan dalam SQL statement.\n",
        "\n",
        "Untuk mengeksekusi SQL statement, kita gunakan fungsi `sql(sqlstatement)` pada spark session."
      ],
      "metadata": {
        "id": "xuijaPcEUz3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select count(*) from app_record\").show()"
      ],
      "metadata": {
        "id": "mEZWLQbtnIHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4624760-de4b-49a4-ec64-540a76f94a10"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|  438557|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from app_record limit 5\").show()"
      ],
      "metadata": {
        "id": "6fLoE47EnMtP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08317b3d-2d4f-4d1b-d81a-23f369390420"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
            "|     ID|CODE_GENDER|FLAG_OWN_CAR|FLAG_OWN_REALTY|CNT_CHILDREN|AMT_INCOME_TOTAL|    NAME_INCOME_TYPE| NAME_EDUCATION_TYPE|  NAME_FAMILY_STATUS|NAME_HOUSING_TYPE|DAYS_BIRTH|DAYS_EMPLOYED|FLAG_MOBIL|FLAG_WORK_PHONE|FLAG_PHONE|FLAG_EMAIL|OCCUPATION_TYPE|CNT_FAM_MEMBERS|\n",
            "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
            "|5008804|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
            "|5008805|          M|           Y|              Y|           0|        427500.0|             Working|    Higher education|      Civil marriage| Rented apartment|    -12005|        -4542|         1|              1|         0|         0|           NULL|            2.0|\n",
            "|5008806|          M|           Y|              Y|           0|        112500.0|             Working|Secondary / secon...|             Married|House / apartment|    -21474|        -1134|         1|              0|         0|         0| Security staff|            2.0|\n",
            "|5008808|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
            "|5008809|          F|           N|              Y|           0|        270000.0|Commercial associate|Secondary / secon...|Single / not married|House / apartment|    -19110|        -3051|         1|              0|         1|         1|    Sales staff|            1.0|\n",
            "+-------+-----------+------------+---------------+------------+----------------+--------------------+--------------------+--------------------+-----------------+----------+-------------+----------+---------------+----------+----------+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita akan melakukan join salah satu kolom dengan data referensi dan melakukan agregasi. Sebelumnya kita buat dataframe referensi dan membuat temporary viewnya"
      ],
      "metadata": {
        "id": "BeWp1FhmLrs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata = (\n",
        "    ('Lower secondary',1),\n",
        "    ('Secondary / secondary special',2),\n",
        "    ('Academic degree',3),\n",
        "    ('Incomplete higher',4),\n",
        "    ('Higher education',5))\n",
        "\n",
        "ref_edu = spark.createDataFrame(mydata).toDF(\"NAME_EDUCATION_TYPE\", \"EDU_LEVEL\")\n",
        "ref_edu.createOrReplaceTempView(\"ref_edu\")\n",
        "spark.sql(\"select * from ref_edu\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py7cZ3yF7INt",
        "outputId": "8b85e7fd-501a-468d-df97-333f00dbd723"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+\n",
            "| NAME_EDUCATION_TYPE|EDU_LEVEL|\n",
            "+--------------------+---------+\n",
            "|     Lower secondary|        1|\n",
            "|Secondary / secon...|        2|\n",
            "|     Academic degree|        3|\n",
            "|   Incomplete higher|        4|\n",
            "|    Higher education|        5|\n",
            "+--------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita lakukan join, agregat, kemudian kita simpan ke tabel"
      ],
      "metadata": {
        "id": "r4kk2JrXMRRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SELECT edu_level, count(1) as number_of_app FROM\n",
        "              (SELECT ref_edu.EDU_LEVEL as edu_level\n",
        "                FROM app_record LEFT JOIN ref_edu\n",
        "                ON app_record.NAME_EDUCATION_TYPE=ref_edu.NAME_EDUCATION_TYPE)\n",
        "             GROUP BY edu_level SORT BY edu_level\"\"\").write.saveAsTable(name=\"aggregated_edu\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "IkuuNd0ynWll"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita tampilkan hasilnya dengan menggunakan perintah `describe formatted`"
      ],
      "metadata": {
        "id": "qV_594qnMeOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"describe formatted aggregated_edu\").show(truncate = False)"
      ],
      "metadata": {
        "id": "ysuMyldWC59p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bc5c32-5dee-41d9-9b31-a20e7f39d1b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+--------------------------------------------+-------+\n",
            "|col_name                    |data_type                                   |comment|\n",
            "+----------------------------+--------------------------------------------+-------+\n",
            "|edu_level                   |bigint                                      |NULL   |\n",
            "|number_of_app               |bigint                                      |NULL   |\n",
            "|                            |                                            |       |\n",
            "|# Detailed Table Information|                                            |       |\n",
            "|Catalog                     |spark_catalog                               |       |\n",
            "|Database                    |default                                     |       |\n",
            "|Table                       |aggregated_edu                              |       |\n",
            "|Created Time                |Sat Oct 07 08:17:51 UTC 2023                |       |\n",
            "|Last Access                 |UNKNOWN                                     |       |\n",
            "|Created By                  |Spark 3.5.0                                 |       |\n",
            "|Type                        |MANAGED                                     |       |\n",
            "|Provider                    |parquet                                     |       |\n",
            "|Location                    |file:/content/spark-warehouse/aggregated_edu|       |\n",
            "+----------------------------+--------------------------------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -l /content/spark-warehouse/aggregated_edu/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk474fUWO-Rz",
        "outputId": "8064ed9c-3c4e-471c-e5f1-3814ddbf11a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 824 Oct  7 08:17 part-00000-3255669c-389b-4c60-beaa-de0defd50b98-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root   0 Oct  7 08:17 _SUCCESS\n"
          ]
        }
      ]
    }
  ]
}