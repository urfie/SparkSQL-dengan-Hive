{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e880ebf7",
   "metadata": {},
   "source": [
    "Import package yang akan kita gunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bed7b",
   "metadata": {},
   "source": [
    "Untuk melakukan koneksi ke Hive, kita perlu menjalankan fungsi enableHiveSupport() pada saat membuat spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5337ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Hive Basics').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fed27",
   "metadata": {},
   "source": [
    "## Menjalankan perintah SHOW dan DESCRIBE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d991c9a",
   "metadata": {},
   "source": [
    "Untuk menjalankan SQL command ke dalam Hive, kita gunakan fungsi `spark.sql()`. Fungsi ini mengembalikan spark DataFrame, sehingga untuk menampilkannya kita perlu memanggil fungsi `show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe database default\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd12b7",
   "metadata": {},
   "source": [
    "## Menjalankan perintah CREATE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bffe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database mytest;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe database mytest\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ae07e",
   "metadata": {},
   "source": [
    "## Membuat managed tabel dari dataframe\n",
    "\n",
    "Kita bisa membuat tabel dari sebuah dataframe. Untuk itu kita buat dataframenya terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Agus','F',100,150,150],['Windy','F',200,150,180],\n",
    "        ['Budi','B',200,100,150],['Dina','F',150,150,130],\n",
    "        ['Bayu','F',50,150,100],['Dedi','B',50,100,100]]\n",
    "\n",
    "kolom = [\"nama\",\"kode_jurusan\",\"nilai1\",\"nilai2\",\"nilai3\"]\n",
    "df = spark.createDataFrame(data,kolom)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c24b4",
   "metadata": {},
   "source": [
    "Untuk menyimpan sebuah dataframe menjadi tabel kita menggunakan perintah `DataFrameWriter.saveAsTable()` ada beberapa parameter yang bisa kita pilih, diantaranya yaitu **mode** yang menyediakan pilihan nilai berupa : *append, overwrite, ignore, error, errorifexists*\n",
    "\n",
    "Untuk contoh ini kita pilih mode *overwrite*, dan kita beri nama tabelnya *mahasiswa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf639632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite') \\\n",
    "         .saveAsTable(\"mytest.mahasiswa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables from mytest\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38870f3",
   "metadata": {},
   "source": [
    "Untuk menampilkan property lengkap dari sebuah tabel, kita gunakan opsi `formatted` atau `extended`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe formatted mytest.mahasiswa\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17eee8",
   "metadata": {},
   "source": [
    "## Melakukan query ke tabel Hive \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f189209",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mytest.mahasiswa\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT  kode_jurusan, count(*) as jumlah_mhs, avg(nilai1) as rata2_nilai1 \n",
    "            FROM mytest.mahasiswa \n",
    "            GROUP BY kode_jurusan\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c59a4f",
   "metadata": {},
   "source": [
    "## Membuat External Tabel dari DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82708592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/hadoop/mydata/mahasiswa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc75d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('overwrite') \\\n",
    "        .option(\"path\", \"hdfs://127.0.0.1:9000/user/hadoop/mydata/mahasiswa\") \\\n",
    "        .saveAsTable(\"mytest.mahasiswa_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb303d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended mytest.mahasiswa_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM mytest.mahasiswa_ext\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68894f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata/mahasiswa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8af96",
   "metadata": {},
   "source": [
    "## Membuat Managed Tabel dengan CREATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"drop table mytest.emp\")\n",
    "#spark.sql(\"drop table mytest.emp_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a901a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS mytest.emp(\n",
    "firstname STRING,\n",
    "lastname STRING,\n",
    "email STRING,\n",
    "gender STRING,\n",
    "age INT,\n",
    "jobtitle STRING,\n",
    "yearsofexperience BIGINT,\n",
    "salary INT,\n",
    "department STRING)\n",
    "STORED AS ORC;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d35b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended mytest.emp\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from mytest.emp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd42e0",
   "metadata": {},
   "source": [
    "## Membuat External Table dengan CREATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac58fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/urfie/SparkSQL-dengan-Hive/raw/main/datasets/emp_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98123044",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/hadoop/mydata/emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a35239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put emp_clean.csv /user/hadoop/mydata/emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe097d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hadoop/mydata/emp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac1fb3",
   "metadata": {},
   "source": [
    "Create external table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373601f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE  EXTERNAL TABLE mytest.emp_ext(\n",
    "firstname STRING,\n",
    "lastname STRING,\n",
    "email STRING,\n",
    "gender STRING,\n",
    "age INT,\n",
    "jobtitle STRING,\n",
    "yearsofexperience BIGINT,\n",
    "salary INT,\n",
    "department STRING)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs://127.0.0.1:9000/user/hadoop/mydata/emp'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d3a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended mytest.emp_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809314ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from mytest.emp_ext\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352efa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mytest.emp_ext limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fb8f8",
   "metadata": {},
   "source": [
    "## Insert into Managed Table from External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73377e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO mytest.emp SELECT * FROM mytest.emp_ext;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from mytest.emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e133eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mytest.emp limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b33895",
   "metadata": {},
   "source": [
    "## Menjalankan fungsi Hive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb902484",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select lower(firstname), lower(lastname), lower(department) from mytest.emp limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154acf35",
   "metadata": {},
   "source": [
    "## Membuat Tabel Dengan Partisi\n",
    "\n",
    "Kita akan gunakan kolom `department` sebagai partisinya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e832abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS mytest.emp_part(\n",
    "firstname STRING,\n",
    "lastname STRING,\n",
    "email STRING,\n",
    "gender STRING,\n",
    "age INT,\n",
    "jobtitle STRING,\n",
    "yearsofexperience BIGINT,\n",
    "salary INT)\n",
    "partitioned by (department string)\n",
    "STORED AS ORC;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe formatted mytest.emp_part\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b056f",
   "metadata": {},
   "source": [
    "Insert data dari tabel non partisi ke tabel dengan partisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"insert overwrite table mytest.emp_part  partition(department) select *  from mytest.emp;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed8fc8",
   "metadata": {},
   "source": [
    "Tampilkan data dari tabel `emp_part`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3133bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from mytest.emp_part\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df5df6",
   "metadata": {},
   "source": [
    "Tampilkan lokasi fisik tabel di hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a59fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/mytest.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9643b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
